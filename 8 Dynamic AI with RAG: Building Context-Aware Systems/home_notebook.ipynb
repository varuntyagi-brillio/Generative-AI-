{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown 1-F1DO6UNkz3ndjSV4kSzu7zcaQJtf5De"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"SQL Revision Notes.pdf\")\n",
    "\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='SQL\n",
      "Revision\n",
      "notes\n",
      "' metadata={'source': 'SQL Revision Notes.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'SQL Revision Notes.pdf', 'page': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SQL\\nRevision\\nnotes\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].page_content[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Different Document Loader, YOUTUBE\n",
    "\n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "# video_url = 'https://www.youtube.com/watch?v=LKCVKw9CzFo'\n",
    "loader = YoutubeLoader(video_id = \"LKCVKw9CzFo\", add_video_info = True)\n",
    "data = loader.load()\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"statistically 96% of the humans watching this video are not using Linux and that's just like really sad because it's a superior free open source operating system but only has a 4% share of the PC market luckily though 96% of the non-human Bots watching this video are using Linux because it is the dominant OS on the server if you're a programmer or developer you need to know Linux that's where your code will eventually run and fail and if you can't SSH into a Linux terminal and fix it you are screwed in today's video you'll learn everything you need to know about Linux by looking at 101 essential Concepts over the next 10 minutes if you survive until the end you should magically grow neck beard and be able to technobabble like an arch user before one can understand Linux though one must recognize what came before it Unix an operating system developed at AT&T Bell labs in the 70s its development led to a standardization called posix or portable operating system interface to ensure that different systems would be compatible with each other its influence remains strong today with Ma ma OS Android FreeBSD and most Linux dros being posix compliant in 1987 an OS called Minix for academic use was developed but redistribution of its code was restricted this inspired a Finnish computer science student named lonus torald to develop Linux in 1991 importantly it's free open- source software licens under GPL 2.0 and by free I mean free is in Freedom it's free to distribute modify and make money off of now what I'm referring to as Linux is in fact not an operating system but rather an operating system kernel it's written in the C programming language and is the black magic that sits between your software applications and the hardware when you hit the power button on your computer the boot loader which is usually grub will load the Linux kernel into random access memory from there it detects hardware and starts the init system which is typically a tool called system D although Alternatives do exist once initialized the kernel will start up applications in user space which will typically bring the user to a login screen as the user starts doing stuff the kernel has a lot of respons ibility it allocates and deallocates memory for processes and can even create virtual memory to use more memory than is physically available by tapping into your hard drive speaking of the hard drive the kernel also provides a virtual file system to interact with files on different systems the fourth extended file system is the most common default on Linux but it's not the only option the kernel also interacts with all these peripheral devices via drivers pretty cool but you can't just walk up and mess around with the kernel and that's because it's surrounded by the cpu's protection ring at ring zero we have the kernel with the highest level of privilege While most of us normies in user space live in ring three with the lowest level of privilege but often you'll want to do something that requires access to the kernel like write a file to the file system and that's where system calls come in in the C code here you'll notice that I'm making a CIS call to write which will transition from ring three to ring zero to Output some text to the console however right itself is not a system call it's actually a rapper provided by GBC which is the G new standard library for C and provides all kinds of rappers for making system calls that can do almost anything on your OS but wait a minute what is gnu it's pronounced gnu and it's a project that predates the Linux kernel itself and was started all the way back in 1983 by Richard stalman it provides all the core utilities for Linux which are all the software utilities that make the kernel useful to humans the best way to start exploring these core Libs is to open up the terminal which is a graphical user interface that allows you to send commands via the shell now they call this thing a shell because it provides a layer of protection between user space and the kernel there are many different flavors of shells but the most common is Bash let's say hello to the Linux kernel by running the ganu shell utility Echo and providing a string argument to it this command takes our message and prints it to the standard output pretty simple but what actually happened under the hood is that a system call was made to the kernel which checked permissions and manage drivers to turn those ones and zeros into pixels on the screen as an end user though I don't care my friend told me about a cool command called touch but I have no idea what it does the cool thing about Linux is that you can pull up the manual for any command by hitting up my main man man it looks like this command is used to create a new file man Isn't that cool let's go ahead and try it out now by creating a new text file it looks like nothing happened but I promise it worked I can prove it by using the ls command to list out the files in this directory and there it is and we can read the file contents with the cat command but again nothing happens because there's no data in this file however there is a bunch bunch of metadata like timestamps that we can access with the stat command is stat rhymes with cat and when we run it we know this file's birth time when it was modified and when it was last accessed that's useful but we can also get more information from the ls command by appending Flags to it like the L flag to list more details and the H flag to make them human readable when we run that command we can now see the exact size of every file and we can also combine flags and Linux to make this command more concise I don't want an empty file though so I'm just going to remove it with the RM command the cool thing about the Linux terminal is that it's really easy to combine commands like I can take Echo and use this angle bracket to redirect its output to a new file in addition I can flip this angle bracket around to also redirect the input of a file that's cool but pipes are even cooler they allow you to take the output of one command and pass it off to another command like for example if we have a log file of our broken code we might first use cat to read that file but then we could pipe the output to sort which would sort it line by line and then unique to remove any duplicates there's so much more we could do just from the terminal but if you find yourself doing the same thing over and over again it might be time to write a bash script in its own dedicated file but to create that file we'll use an interactive text editor if you have a few years to spare you could try learning Vim or if you have no life at all you could try emac but I'm going to create this file with Nano a minimal text editor built into most Linux dros at the top of the file We'll add a shebang that tells Linux to use the bash interpreter then we can add as much bash code as we want here like we might use Echo and then read to read a value from the standard input and then Echo to once again Echo it back and now if we save this file we can execute it by simply entering the file path in the terminal and that brings us to the halfway mark unfortunately 100 topics is not nearly enough to cover the entire Linux ecosystem if your goal is to become an apex Alpha Linux gigachad and check out my brand new full Linux course for fireship pro members the course contains over 30 videos of Hands-On Linux topics along with quizzes for regular dopamine hits that will help you master the fundamentals of Linux quickly but most importantly it will teach you how to spin up your own virtual private server to self-host your own applications but one major drawback of using Linux is that it might trigger an existential crisis and you might ask yourself who am I when you enter that command it's going to return your Linux username in addition every user has a unique uid that can be viewed with the ID command my ID is 1000 but there is one special user with a uid of zero called root AKA admin super user or daddy root has the highest level of privilege and you can switch to the root user with the Su command or prefix any command with pseudo to run it with elevated privilege any user can be granted pseudo privilege and you can check your privilege right now by running pseudo flag l in addition to users Linux also has groups groups have group IDs and make it easier to manage permissions for multiple users before we talk about permissions though let's explore the file system by default we're in our home directory which is like a personal workspace for the user you're logged in as we might make a new directory here using the make dur command then use CD to change directories into it now run PWD to print the current working directory but now let's venture outside of our home into the root of the file system by running cd/ if you hit LS here you'll find a bunch of critical directories that you need to know aot like boot contains the Linux kernel itself Dev contains external devices like hard drives Etc contains config files and VAR contains log files the most interesting directory here though is bin which holds your binaries and spin for system binaries you see when you run a command like LS Linux looks for an executable binary on your system to execute the thing is binaries not only live here but also under the user system Resources directory and potentially anywhere you want on the file system and that begs the question how does Linux know where to find the right binary well that's where path comes in it's a special environment variable that contains paths to directories separated by colons when you enter a command Linux will search through the path for a matching binary in each directory and execute the first one it finds its value is set on the system by default but it's common to customize it by using the export keyword which will set the value for an environment variable the most common technique is to update the path for an individual user by customizing The Bash RC file which itself is a script that will run before every terminal session and now that we're inside this file we can also do things like customize the PS1 environment variable to change the terminal prompt to look more like a mega hardcore hacker when coding at Starbucks which is a proven way to attract a mate as a Linux user but now it's time to talk about file permissions use LS flag L on any file to view permissions and notice these cryptic nine characters these are called symbolic permissions the first triplet represents the owner the middle the group and the last triplet is for everyone else each one contains a letter that represents read write and execute privileges if the letter is present it means access granted but if there's a dash it means permission denied these can also be represented as numbers in o notation for example 777 lets anybody do anything to a file I know Triple 7 is good on slot machines but in Linux it's generally a bad idea because you want to always follow the principle of lease privilege Grant access to things only when necessary and trust no one now you can modify the permissions on a file with the chamod command like here I'm using it to Grant read access to a document for everybody we can also change the owner of a file with Chon or assign groups with ch group and now that we know what all these things mean permissions aren't so cryp now anytime you run a command or execute a program it creates a process on the CPU which is managed by the Linux kernel you can view these processes with a command and notice how each one has a unique process ID along with the user who created it or better yet use htop to get an interactive breakdown of processes that can be filtered some of these are just system demons that run in the background we all have our demons in fact if you have a long running script you can even create your own background process by adding an Amper sand to the end of it or if you want a a script run on a specific schedule like a reminder to do something at 4:20 p.m. today you can accomplish that by adding your script to the cron tab that's cool but occasionally you'll have a bad process that needs to be killed the kill command can do that by gracefully sending a Sig term signal to the process if that doesn't do the trick though use the nine flag to forcefully kill it with Sig kill we've barely scratched the surface but so far everything we've looked at is pretty standard on most Linux machines other utilities you should know about include GP for searching through text said for modifying tax gzip for making files smaller and tar for archiving directories but the Linux experience varies wildly when talking about different dros a Linux distribution is just a complete operating system built on the Linux kernel and each drro has a highly opinionated set of default software for their target audience some are designed for beginners others for hardcore hackers and everything in between dros can have different package managers to install new software like a yum and Pac-Man and they might also have different release schedules like some have a predictable fixed release date While others have ruling releases that keep their software on The Cutting Edge at all times and one thing we didn't even talk about is desktop environments if using Linux as a PC your Dro will have a default desktop environment like gnome or KD plasma and that makes a huge difference in the experience some distro families you should know about include slackware the Original Gangster from the 9s Debian the most popular Dro overall famous for its open philosophy and ease of use red hat the dro of choice and Enterprise for its long-term support plans and finally the arch family if someday you find yourself saying I use Arch by the way unironically it means you've embrace the Paradox of complexity and simplicity and your operating system is no longer just a tool but a reflection of your own dominance and Mastery over the digital world thanks for watching check out the full course if you want to go deeper and I will see you in the next one\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat Is Retrieval-Augmented Generation aka RAG | NVIDIA Blogs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tArtificial Intelligence Computing Leadership from NVIDIA\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch for:\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\nToggle Search\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\nSearch for:\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n \\n\\n\\n \\n \\n\\n \\n \\n\\n \\n \\n\\n \\n \\n\\n\\n \\n \\n\\n \\n \\n\\n \\n \\n\\n \\n \\n\\n\\n \\n \\n\\n\\n\\n\\n\\n\\n\\nPrivacy Policy\\n\\n\\nManage My Privacy\\n\\n\\nLegal\\n\\n\\nAccessibility\\n\\n\\nProduct Security\\n\\n\\nContact\\n\\n\\n\\n\\t\\t\\t'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WEBPAGE LOADER\n",
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\")\n",
    "docs = loader.load()\n",
    "\n",
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CharacterTextSplitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "char_text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\",\n",
    "    chunk_size = 10,\n",
    "    chunk_overlap = 2\n",
    ")\n",
    "\n",
    "# char_text_splitter = CharacterTextSplitter(chunk_size=10, chunk_overlap=2, separator=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abcdefghij', 'ijklmnopqr', 'qrstuvacwx', 'wxyz']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"abcdefghijklmnopqrstuvacwxyz\"\n",
    "print(char_text_splitter.split_text(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abcdefghij', 'ijkl mnopq', 'pqrstuvacw', 'cwxyz']\n"
     ]
    }
   ],
   "source": [
    "text1 = \"abcdefghijkl mnopqrstuvacwxyz\"\n",
    "print(char_text_splitter.split_text(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 11, which is longer than the specified 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abcdefghijk', 'lmnopqrs', 'tuvwxyz', 'abcdefg']\n"
     ]
    }
   ],
   "source": [
    "char_text_splitter = CharacterTextSplitter(\n",
    "    separator = \" \",\n",
    "    chunk_size = 10,\n",
    "    chunk_overlap = 2\n",
    ")\n",
    "\n",
    "text2 = \"abcdefghijk lmnopqrs tuvwxyz abcdefg\"\n",
    "print(char_text_splitter.split_text(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abcdefghijklmnopqrstuvacwxyz']\n"
     ]
    }
   ],
   "source": [
    "# RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 200,\n",
    "    chunk_overlap = 20\n",
    ")\n",
    "\n",
    "text1 = \"abcdefghijklmnopqrstuvacwxyz\"\n",
    "print(r_splitter.split_text(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks:  ['What I Worked On\\n\\nFebruary 2021', \"Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still\", 'and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.', 'The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401', \"district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these\", 'with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.']\n",
      "Length of chunks:  6\n"
     ]
    }
   ],
   "source": [
    "some_text = \"\"\"What I Worked On\n",
    "\n",
    "February 2021\n",
    "\n",
    "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
    "\n",
    "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n",
    "\"\"\"\n",
    "\n",
    "chunk = r_splitter.split_text(some_text)\n",
    "print(\"Chunks: \", chunk)\n",
    "print(\"Length of chunks: \", len(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What I Worked On\n",
      "\n",
      "February 2021\n",
      "31\n",
      "-------------------------------------------\n",
      "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still\n",
      "198\n",
      "-------------------------------------------\n",
      "and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
      "158\n",
      "-------------------------------------------\n",
      "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401\n",
      "198\n",
      "-------------------------------------------\n",
      "district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these\n",
      "197\n",
      "-------------------------------------------\n",
      "with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\n",
      "142\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunk:\n",
    "  print(chunk)\n",
    "  print(len(chunk))\n",
    "  print(\"-------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding using OpenAI model\n",
    "import openai\n",
    "import config\n",
    "import os\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version = \"2023-03-15-preview\",\n",
    "    model=\"text-embedding-3-small\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"i like dogs\"\n",
    "sentence2 = \"i like canines\"\n",
    "sentence3 = \"the weather is ugly outside\"\n",
    "\n",
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "embedding3 = embedding.embed_query(sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8338763953819934"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(embedding1, embedding2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown 1yVbhJWh4L1unDbDT4APOusTXlwic7aE9\n",
    "# !gdown 1-F1DO6UNkz3ndjSV4kSzu7zcaQJtf5De"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loaders = [\n",
    "    PyPDFLoader(\"PA - Consolidated lecture notes.pdf\"),\n",
    "    PyPDFLoader(\"SQL Revision Notes.pdf\"),\n",
    "]\n",
    "\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1500,\n",
    "    chunk_overlap = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS, vectordb for saving embeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(\n",
    "    splits,\n",
    "    embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "print(db.index.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus\n",
      "metric\n",
      "/\n",
      "North\n",
      "Star\n",
      "metric\n",
      "-\n",
      "This\n",
      "is\n",
      "the\n",
      "single\n",
      "most\n",
      "important\n",
      "measure\n",
      "of\n",
      "success\n",
      "that\n",
      "matters\n",
      "a\n",
      "lot\n",
      "to\n",
      "a\n",
      "company .\n",
      "A\n",
      "North\n",
      "Star\n",
      "Metric\n",
      "(NSM)\n",
      "should\n",
      "be:\n",
      "●\n",
      "A\n",
      "direct\n",
      "reflection\n",
      "of\n",
      "the\n",
      "company’ s\n",
      "mission\n",
      "●\n",
      "An\n",
      "indicator\n",
      "of\n",
      "how\n",
      "a\n",
      "company\n",
      "brings\n",
      "value\n",
      "to\n",
      "its\n",
      "customers.\n",
      "●\n",
      "The\n",
      "only\n",
      "one\n",
      "of\n",
      "its\n",
      "kind.\n",
      "(Avoid\n",
      "having\n",
      "multiple\n",
      "NSMs\n",
      "as\n",
      "this\n",
      "tends\n",
      "to\n",
      "create\n",
      "complexity\n",
      "and\n",
      "confusion)\n",
      "●\n",
      "The\n",
      "answer\n",
      "to\n",
      "the\n",
      "following\n",
      "question:\n",
      "What\n",
      "is\n",
      "the\n",
      "one\n",
      "metric\n",
      "that\n",
      "best\n",
      "represents\n",
      "the\n",
      "desired\n",
      "outcome\n",
      "of\n",
      "your\n",
      "company?\n",
      "Which\n",
      "metric,\n",
      "if\n",
      "it\n",
      "were\n",
      "to\n",
      "increase\n",
      "today ,\n",
      "would\n",
      "most\n",
      "accelerate\n",
      "my\n",
      "business’\n",
      "flywheel?\n",
      "Level\n",
      "1\n",
      "metric\n",
      "/\n",
      "Primary\n",
      "metric\n",
      "-\n",
      "●\n",
      "Primary\n",
      "metrics\n",
      "depict\n",
      "the\n",
      "desired\n",
      "outcome\n",
      "of\n",
      "a\n",
      "particular\n",
      "product,\n",
      "team,\n",
      "or\n",
      "initiative.\n",
      "This\n",
      "is\n",
      "unlike\n",
      "the\n",
      "NSM,\n",
      "which\n",
      "represents\n",
      "the\n",
      "desired\n",
      "outcome\n",
      "of\n",
      "the\n",
      "company\n",
      "as\n",
      "a\n",
      "whole.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is north star metric\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(metadata={'source': 'PA - Consolidated lecture notes.pdf', 'page': 16}, page_content='Focus\\nmetric\\n/\\nNorth\\nStar\\nmetric\\n-\\nThis\\nis\\nthe\\nsingle\\nmost\\nimportant\\nmeasure\\nof\\nsuccess\\nthat\\nmatters\\na\\nlot\\nto\\na\\ncompany .\\nA\\nNorth\\nStar\\nMetric\\n(NSM)\\nshould\\nbe:\\n●\\nA\\ndirect\\nreflection\\nof\\nthe\\ncompany’ s\\nmission\\n●\\nAn\\nindicator\\nof\\nhow\\na\\ncompany\\nbrings\\nvalue\\nto\\nits\\ncustomers.\\n●\\nThe\\nonly\\none\\nof\\nits\\nkind.\\n(Avoid\\nhaving\\nmultiple\\nNSMs\\nas\\nthis\\ntends\\nto\\ncreate\\ncomplexity\\nand\\nconfusion)\\n●\\nThe\\nanswer\\nto\\nthe\\nfollowing\\nquestion:\\nWhat\\nis\\nthe\\none\\nmetric\\nthat\\nbest\\nrepresents\\nthe\\ndesired\\noutcome\\nof\\nyour\\ncompany?\\nWhich\\nmetric,\\nif\\nit\\nwere\\nto\\nincrease\\ntoday ,\\nwould\\nmost\\naccelerate\\nmy\\nbusiness’\\nflywheel?\\nLevel\\n1\\nmetric\\n/\\nPrimary\\nmetric\\n-\\n●\\nPrimary\\nmetrics\\ndepict\\nthe\\ndesired\\noutcome\\nof\\na\\nparticular\\nproduct,\\nteam,\\nor\\ninitiative.\\nThis\\nis\\nunlike\\nthe\\nNSM,\\nwhich\\nrepresents\\nthe\\ndesired\\noutcome\\nof\\nthe\\ncompany\\nas\\na\\nwhole.'),\n",
       " 0.92471546)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_and_scores = db.similarity_search_with_score(query)\n",
    "docs_and_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(metadata={'source': 'SQL Revision Notes.pdf', 'page': 1}, page_content='Relational\\nDatabase\\n●\\nIt\\nis\\na\\ncollection\\nof\\ninterrelated\\ntables.\\nThe\\ndata\\nwithin\\nthese\\ntables\\nhave\\nrelationships\\nwith\\none\\nanother.\\nExample:\\nMicrosoft\\nSQL\\nServer,\\nMySQL,\\netc.\\n●\\nA\\nrelational\\ndatabase\\nis\\nstructured.\\nNon-relational\\nDatabase\\n●\\nIt\\nis\\na\\nkind\\nof\\ndatabase\\nthat\\ndoesn’t\\nuse\\ntables,\\nfields,\\nand\\ncolumns\\nof\\nstructured\\ndata.\\nExamples:\\nMongoDB,\\nApache\\nCassandra,\\netc.\\n●\\nA\\nnon-relational\\ndatabase\\nis\\nsemi-structured\\nor\\nunstructured.\\nManagement\\nsystem\\n●\\nA\\nset\\nof\\noperations\\nthat\\nhelp\\nus\\nto\\nmanage\\nthe\\ndatabase.\\n●\\nSome\\nof\\nthese\\noperations\\nare\\ncalled\\nCRUD\\noperations.\\n○\\nC\\n-\\ncreate\\n○\\nR\\n-\\nread\\n○\\nU\\n-\\nupdate\\n○\\nD\\n-\\ndelete\\nTypes\\nof\\nrelationships\\nin\\nRDBMS\\nThere\\nare\\nthree\\ntypes\\nof\\nrelationships\\nthat\\ncan\\nbe\\npresent\\nbetween\\ntables:\\n●\\nOne-to-one\\nrelationship\\noccurs\\nwhen\\neach\\nrow\\nin\\nTable\\n1\\nhas\\nonly\\none\\nrelated\\nrow\\nin\\nTable\\n2.\\n●\\nOne-to-many\\noccurs\\nwhen\\none\\nrecord\\nin\\nTable\\n1\\nis\\nrelated\\nto\\none\\nor\\nmore\\nrecords\\nin\\nTable\\n2.\\n●\\nMany-to-many\\noccurs\\nwhen\\nmultiple\\nrecords\\nin\\none\\ntable\\nare\\nrelated\\nto\\nmultiple\\nrecords\\nin\\nanother\\ntable.\\nPrimary\\nkey:\\nA\\nprimary\\nkey\\nis\\na\\nfield\\nin\\na\\ntable\\nthat\\nuniquely\\nidentifies\\neach\\nrow/record\\nin\\na\\ndatabase\\ntable.\\nPrimary\\nkeys\\nmust\\ncontain\\nunique\\nvalues.\\nA\\nprimary\\nkey\\ncolumn\\ncannot\\nhave\\nNULL\\nvalues.\\nForeign\\nKey:\\nA\\nforeign\\nkey\\nis\\na\\ncolumn\\nor\\na\\ncombination\\nof\\ncolumns\\nwhose\\nvalues\\nmatch\\na\\nPrimary\\nKey\\nin\\na\\ndifferent\\ntable.\\nIt\\nis\\na\\ncolumn\\nused\\nto\\nlink\\ntwo\\ntables\\ntogether.\\nDatabase\\nschema\\n●\\nA\\ndatabase\\nschema\\nserves\\nas\\na\\nblueprint\\nfor\\nthe\\nshape\\nand\\nformat\\nof\\ndata\\nwithin\\na\\ndatabase.'),\n",
       " 1.2602584)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is primary key\"\n",
    "docs_and_scores = db.similarity_search_with_score(query)\n",
    "\n",
    "docs_and_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(metadata={'source': 'PA - Consolidated lecture notes.pdf', 'page': 11}, page_content=\"1.\\n%\\nof\\nusers\\nreturning\\nto\\nview\\nsaved\\ncontent\\norganically\\n(on\\ntheir\\nown)\\n-\\nUser\\nknows\\nwhere\\nto\\nﬁnd\\nSaved\\nitems\\nand\\nknows\\nhow\\nto\\nwork\\nwith\\nit.\\n2.\\n%\\nof\\nusers\\nreturning\\ninorganically\\n(i.e.,\\nreminded\\nby\\n3.\\nFacebook\\nto\\nview\\nsaved\\ncontent).\\n2.\\nAcquisition:\\na.\\n#\\nof\\nnew\\nclients\\nwho\\nwant\\nto\\nadvertise\\nwith\\nFacebook.\\nb.\\nIncreased\\nspending\\nof\\nexisting\\nclients\\nwith\\nFacebook\\nsince\\nthe\\nlaunch\\nof\\nthe\\nsave\\nfeature.\\n3.\\nActivation:\\na.\\nAdoption:\\ni.\\n%\\nof\\ntotal\\nposts\\nsaved\\n(#\\nSaved\\nPosts\\n/\\n#\\nTotal\\nPosts)-Indicates\\nthe\\nadoption\\nrate\\nof\\nthe\\nsave\\nfeature\\n-\\n#\\npeople\\nusing\\nthe\\nsave\\nfeature\\nactively.\\n4.\\nEngagement:\\na.\\nAverage\\nnumber\\nof\\nlikes,\\ncomments,\\nshares\\nper\\nsaved\\npost\\non\\na\\ndaily,\\nweekly,\\nand\\nmonthly\\nbasis-\\nindicates\\nthe\\nuser\\nengagement\\non\\na\\nbroader\\nlevel.\\nCompare\\nthis\\nwith\\na\\ngeneral\\npost\\nwith\\nno\\nsave\\nfeature\\nand\\nsee\\nif\\nthe\\nengagement\\nis\\nmore\\nwith\\nthe\\nnew\\nfeature.\\nb.\\n%of\\nSaved\\nitems\\nthat\\nthe\\nuser\\nopens\\nfrom\\nthe\\nSaved\\npage.\\nc.\\nAmount\\nof\\ntime\\nspent\\non\\na\\npage,\\nafter\\nopening\\nit\\nfrom\\nthe\\nSaved\\npage.\\nd.\\nThe\\naverage\\namount\\nof\\ntime\\nit\\ntook\\na\\nuser\\nfrom\\nSaving\\nan\\nitem\\nto\\nopening\\nit\\nagain.\\n5.\\nRevenue:\\na.\\n%\\nrevenue\\nincrease\\njust\\nbased\\non\\nclicks\\nand\\nimpressions\\nmade\\nthrough\\nthe\\nfunnel\\nthat\\nincludes\\nSaved\\nitems.\\n6.\\nGuardrail\\nMetrics:\\n(Along\\nwith\\nsuccess\\ndeﬁning\\nmetrics,\\nit's\\nalso\\nimportant\\nto\\ndeﬁne\\nguardrail\\nmetrics)\"),\n",
       " 0.8830776)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"list out some engagement metrics\"\n",
    "docs_and_scores = db.similarity_search_with_score(query,k=3)\n",
    "\n",
    "docs_and_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Document(metadata={'source': 'PA - Consolidated lecture notes.pdf', 'page': 11}, page_content=\"1.\\n%\\nof\\nusers\\nreturning\\nto\\nview\\nsaved\\ncontent\\norganically\\n(on\\ntheir\\nown)\\n-\\nUser\\nknows\\nwhere\\nto\\nﬁnd\\nSaved\\nitems\\nand\\nknows\\nhow\\nto\\nwork\\nwith\\nit.\\n2.\\n%\\nof\\nusers\\nreturning\\ninorganically\\n(i.e.,\\nreminded\\nby\\n3.\\nFacebook\\nto\\nview\\nsaved\\ncontent).\\n2.\\nAcquisition:\\na.\\n#\\nof\\nnew\\nclients\\nwho\\nwant\\nto\\nadvertise\\nwith\\nFacebook.\\nb.\\nIncreased\\nspending\\nof\\nexisting\\nclients\\nwith\\nFacebook\\nsince\\nthe\\nlaunch\\nof\\nthe\\nsave\\nfeature.\\n3.\\nActivation:\\na.\\nAdoption:\\ni.\\n%\\nof\\ntotal\\nposts\\nsaved\\n(#\\nSaved\\nPosts\\n/\\n#\\nTotal\\nPosts)-Indicates\\nthe\\nadoption\\nrate\\nof\\nthe\\nsave\\nfeature\\n-\\n#\\npeople\\nusing\\nthe\\nsave\\nfeature\\nactively.\\n4.\\nEngagement:\\na.\\nAverage\\nnumber\\nof\\nlikes,\\ncomments,\\nshares\\nper\\nsaved\\npost\\non\\na\\ndaily,\\nweekly,\\nand\\nmonthly\\nbasis-\\nindicates\\nthe\\nuser\\nengagement\\non\\na\\nbroader\\nlevel.\\nCompare\\nthis\\nwith\\na\\ngeneral\\npost\\nwith\\nno\\nsave\\nfeature\\nand\\nsee\\nif\\nthe\\nengagement\\nis\\nmore\\nwith\\nthe\\nnew\\nfeature.\\nb.\\n%of\\nSaved\\nitems\\nthat\\nthe\\nuser\\nopens\\nfrom\\nthe\\nSaved\\npage.\\nc.\\nAmount\\nof\\ntime\\nspent\\non\\na\\npage,\\nafter\\nopening\\nit\\nfrom\\nthe\\nSaved\\npage.\\nd.\\nThe\\naverage\\namount\\nof\\ntime\\nit\\ntook\\na\\nuser\\nfrom\\nSaving\\nan\\nitem\\nto\\nopening\\nit\\nagain.\\n5.\\nRevenue:\\na.\\n%\\nrevenue\\nincrease\\njust\\nbased\\non\\nclicks\\nand\\nimpressions\\nmade\\nthrough\\nthe\\nfunnel\\nthat\\nincludes\\nSaved\\nitems.\\n6.\\nGuardrail\\nMetrics:\\n(Along\\nwith\\nsuccess\\ndeﬁning\\nmetrics,\\nit's\\nalso\\nimportant\\nto\\ndeﬁne\\nguardrail\\nmetrics)\"),\n",
       " 0.8830776)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"list out some engagement metrics\"\n",
    "docs_and_scores = db.similarity_search_with_score(query,k=3)\n",
    "\n",
    "docs_and_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the created DB\n",
    "\n",
    "db.save_local(\"faiss_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the saved Vectordb\n",
    "\n",
    "new_db = FAISS.load_local(\"faiss_db\", embedding, allow_dangerous_deserialization=True)\n",
    "\n",
    "docs = new_db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question Answering\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version = \"2023-03-15-preview\",\n",
    "    temperature = 0.2, \n",
    "    model = \"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    retriever = new_db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/Documents/MS/GenAI/venv_gen/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some engagement metrics:\n",
      "\n",
      "1. Average number of likes, comments, and shares per saved post on a daily, weekly, and monthly basis.\n",
      "2. Percentage of saved items that the user opens from the Saved page.\n",
      "3. Amount of time spent on a page after opening it from the Saved page.\n",
      "4. Average amount of time it took a user from saving an item to opening it again.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using Prompt Template\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer and dont find it in the given context, just say that you don't know , don't try to make up an answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm =llm,\n",
    "    retriever = db.as_retriever(),\n",
    "    return_source_documents = True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/Documents/MS/GenAI/venv_gen/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\n",
    "    \"query\": query\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some engagement metrics based on the provided context:\n",
      "\n",
      "1. **Average number of likes, comments, shares per saved post** on a daily, weekly, and monthly basis.\n",
      "2. **Percentage of Saved items** that the user opens from the Saved page.\n",
      "3. **Amount of time spent on a page** after opening it from the Saved page.\n",
      "4. **Average time taken** by a user from saving an item to opening it again.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'PA - Consolidated lecture notes.pdf', 'page': 11}, page_content=\"1.\\n%\\nof\\nusers\\nreturning\\nto\\nview\\nsaved\\ncontent\\norganically\\n(on\\ntheir\\nown)\\n-\\nUser\\nknows\\nwhere\\nto\\nﬁnd\\nSaved\\nitems\\nand\\nknows\\nhow\\nto\\nwork\\nwith\\nit.\\n2.\\n%\\nof\\nusers\\nreturning\\ninorganically\\n(i.e.,\\nreminded\\nby\\n3.\\nFacebook\\nto\\nview\\nsaved\\ncontent).\\n2.\\nAcquisition:\\na.\\n#\\nof\\nnew\\nclients\\nwho\\nwant\\nto\\nadvertise\\nwith\\nFacebook.\\nb.\\nIncreased\\nspending\\nof\\nexisting\\nclients\\nwith\\nFacebook\\nsince\\nthe\\nlaunch\\nof\\nthe\\nsave\\nfeature.\\n3.\\nActivation:\\na.\\nAdoption:\\ni.\\n%\\nof\\ntotal\\nposts\\nsaved\\n(#\\nSaved\\nPosts\\n/\\n#\\nTotal\\nPosts)-Indicates\\nthe\\nadoption\\nrate\\nof\\nthe\\nsave\\nfeature\\n-\\n#\\npeople\\nusing\\nthe\\nsave\\nfeature\\nactively.\\n4.\\nEngagement:\\na.\\nAverage\\nnumber\\nof\\nlikes,\\ncomments,\\nshares\\nper\\nsaved\\npost\\non\\na\\ndaily,\\nweekly,\\nand\\nmonthly\\nbasis-\\nindicates\\nthe\\nuser\\nengagement\\non\\na\\nbroader\\nlevel.\\nCompare\\nthis\\nwith\\na\\ngeneral\\npost\\nwith\\nno\\nsave\\nfeature\\nand\\nsee\\nif\\nthe\\nengagement\\nis\\nmore\\nwith\\nthe\\nnew\\nfeature.\\nb.\\n%of\\nSaved\\nitems\\nthat\\nthe\\nuser\\nopens\\nfrom\\nthe\\nSaved\\npage.\\nc.\\nAmount\\nof\\ntime\\nspent\\non\\na\\npage,\\nafter\\nopening\\nit\\nfrom\\nthe\\nSaved\\npage.\\nd.\\nThe\\naverage\\namount\\nof\\ntime\\nit\\ntook\\na\\nuser\\nfrom\\nSaving\\nan\\nitem\\nto\\nopening\\nit\\nagain.\\n5.\\nRevenue:\\na.\\n%\\nrevenue\\nincrease\\njust\\nbased\\non\\nclicks\\nand\\nimpressions\\nmade\\nthrough\\nthe\\nfunnel\\nthat\\nincludes\\nSaved\\nitems.\\n6.\\nGuardrail\\nMetrics:\\n(Along\\nwith\\nsuccess\\ndeﬁning\\nmetrics,\\nit's\\nalso\\nimportant\\nto\\ndeﬁne\\nguardrail\\nmetrics)\")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"source_documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": \"what is backpropogation in neural networks\"})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
